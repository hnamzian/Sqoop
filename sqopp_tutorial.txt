hostname -f


mysql -u root -p
mysql -u retail_dba -p 


hadoop fs -mkdir /user/cloudera/sqoop_import


sqoop list-databases \
--connect "jdbc:mysql://quickstart.cloudera:3306" \
--username retail_dba \
--password cloudera


sqoop list-tables \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera


sqoop eval \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--query "select * from departments"


sqoop eval \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--query "select * from customers limit 10"


--import tables from mysql path into hdfs as a text file
sqoop import-all-tables \
-m 12 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--as-textfile \
--warehouse-dir=/user/cloudera/sqoop_import


--see the data
hadoop fs -tail /user/cloudera/sqoop_import/categories/part-m-00011


--see the data
hadoop fs -tail /user/cloudera/sqoop_import/categories/part-m-*


--read the data and count number of lines
hadoop fs -cat /user/cloudera/sqoop_import/order_items/part-m-* | wc -l


--and validate the above result by run a query using mysql
mysql -u retail_dba -p -e "select count(1) from retail_db.order_items"


--as sqoop provides query evaluation without the need of using mysql
--you can use sqoop to run a query directly on mysql tables
sqoop eval \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--query "select count(1) from order_items"


--import tables into hive using sqoop
--This is how sqoop imports job works:
--1-sqoop creates/imports data in tmp dir(HDFS) which is user's home dir(in your case it is /user/cloudera).
--2-Then copy data to its actual hive location (i.e., /user/hive/wearhouse.
--3-This categories dir should have exist before you ran import statements. so delete that dir or rename it if its important.
--hadoop fs -rmr /user/cloudera/categories
--OR
--hadoop fs -mv /user/cloudera/categories /user/cloudera/categories_1
--if it hangs, it might be due to zookeeper not working
sqoop import-all-tables \
--num-mappers 1 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--hive-home /user/hive/warehouse \
--hive-database retail_stage \
--hive-import \
--hive-overwrite \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--create-hive-table \
--outdir java_files 


hadoop fs -ls /user/hive/warehouse-dir
hive -e "show tables;"


--import single table into hive using sqoop
--there is 2 ways:
--1-create table while importing table
--2-create table and the import data inot the table

--go to the hive and create a database named sqoop_import and then create a table named departments
create database sqoop_import;

create table departments (
department_id int,
department_name string);


--the 1st way
sqoop import \
--num-mappers 1 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--fields-terminated-by '|' \
--lines-terminated-by '\n' \
--hive-home /user/hive/warehouse/ \
--hive-table sqoop_import.departments \
--hive-import \
--hive-overwrite \
--outdir javafiles


--now see the table using hive query
select * from departments;


--the 2nd way
sqoop import \
--num-mappers 1 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--fields-terminated-by '|' \
--lines-terminated-by '\n' \
--hive-home /user/hive/warehouse/ \
--hive-import \
--hive-overwrite \
--hive-table sqoop_import.departments_test \
--create-hive-table \
--outdir javafiles

--now see the table using hive query
select * from departments_test;


--run the following commands and you see that the contents of table are duplicated
sqoop import \
--num-mappers 1 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--fields-terminated-by '|' \
--lines-terminated-by '\n' \
--hive-home /user/hive/warehouse/ \
--hive-import \
--hive-table sqoop_import.departments \
--outdir javafiles

--now see the table using hive query
select * from departments;


--import table into hdfs incrementally using Sqoop
--append mode is not yet supported for hive!!!

hadoop fs -ls /user/cloudera/sqoop_import/departments/
hadoop fs -cat /user/cloudera/sqoop_import/departments/part-m-*

sqoop import \
--num-mappers 1 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--append \
--target-dir /user/cloudera/sqoop_import/departments \
--where "department_id>5" \
--outdir java_files

hadoop fs -ls /user/cloudera/sqoop_import/departments/
hadoop fs -cat /user/cloudera/sqoop_import/departments/part-m-*

--OR

hadoop fs -ls /user/cloudera/sqoop_import/departments/
hadoop fs -cat /user/cloudera/sqoop_import/departments/part-m-*

sqoop import \
--num-mappers 1 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--incremental append \
--check-column department_id \
--last-value 5 \
--target-dir /user/cloudera/sqoop_import/departments \
--outdir java_files

hadoop fs -ls /user/cloudera/sqoop_import/departments/
hadoop fs -cat /user/cloudera/sqoop_import/departments/part-m-*



sqoop import \
--num-mappers 1 \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--incremental append \
--check-column department_id \
--last-value 5 \
--hive-import \
--target-dir /user/hive/warehouse/sqoop_import/departments \
--outdir java_files


--Export
--frist create a table in a database in mysql
mysql> create table order_items_export as select * from order_items where 1=2;

--now export a tbale from hdfs into mysql
sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table order_items_export \
--export-dir /user/cloudera/sqoop_import/order_items \
--batch \
--outdir java_files


--Export in update mode
--First compare the results of the following commands:

hadoop fs -cat /user/cloudera/sqoop_import/departments/*

sqoop eval \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--query "select * from departments"

--create a new file named departmenst_export
nano departments_export
--and insert these rows to the file
7,fanshop
9000,testing export

--copy this file into a new directory in hdfs
hadoop fs -mkdir /user/cloudera/sqoop_import/department_export
hadoop fs -put departments_export /user/cloudera/sqoop_import/departments_export
hadoop fs -mv /user/cloudera/sqoop_import/department_export/departments_export \
              /user/cloudera/sqoop_import/department_export/departments_export.csv 

--export the department_export
sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--export-dir /user/cloudera/sqoop_import/department_export/departments_export.csv \
--batch \
--update-key department_id \
--update-mode updateonly

--and now check the table departments in mysql
select * from departments


--upsert table
sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--export-dir /user/cloudera/sqoop_import/department_export/departments_export.csv \
--batch \
--update-key department_id \
--update-mode allowinsert

--and now check the table departments in mysql
select * from departments


--import while formatting
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--target-dir /user/cloudera/sqoop_import/department_foramt \
--enclosed-by \" \
--fields-terminated-by "|" \
--lines-terminated-by ":" 

hadoop fs -cat /user/cloudera/sqoop_import/department_foramt/*

--import into hive and formatting
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--hive-import \
--hive-home /user/hive/warehouse \
--hive-table department_format \
--create-hive-table \
--enclosed-by \" \
--fields-terminated-by "|" \
--outdir java_files

hadoop fs -cat /user/hive/warhouse/department_foramt/*

hive -e "select * from department_format"


--fill null values
create table department_null(department_id int, department_name varchar(30));
insert into department_null select * from departments;
insert into department_null value (8, null);
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
-m 1 \
--table department_null \
--hive-import \
--hive-home /user/hive/warehouse \
--hive-table department_null \
--create-hive-table \
--null-string "nullstring" \
--null-non-string 999 \
--outdir java_files 


--Export while formatting
--first you can empty department_null table created before in mysql
truncate table department_null;

sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--num-mappers 2 \
--batch \
--table department_null \
--export-dir /user/hive/warehouse/department_null \
--input-null-non-string 999 \
--input-fields-terminated-by '\0001' \
--input-lines-terminated-by '\n' \
--input-null-string 'nullstring'
--outdir java_files 












